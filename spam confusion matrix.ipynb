{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZcJhEBxqrxfsOTcBu/OsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhengwu123/Machine_learning_group_project/blob/master/spam%20confusion%20matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_jepxO-p3_e"
      },
      "source": [
        "Question 1. (k-NN classifier and 10-fold cross validation)\n",
        "We want to train a k-NN classifier for the Iris Data Set. Use 10-fold cross validation to select a good\n",
        "k from [1,50] for the k-NN classifier. Submit k and the source code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z9lKKxFp5x4"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from pydotplus import graph_from_dot_data\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "# get iris data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrfvjWKLqbhV"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCOt1XqsEch4"
      },
      "source": [
        "for train_index, test_index in kf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7StIgemGu7Q"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "#set fold to 10\n",
        "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljbxe6fsHUwO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#find best k\n",
        "#reference:\n",
        "#https://www.ritchieng.com/machine-learning-cross-validation/\n",
        "k_range = range(1, 50)\n",
        "# empty list to store scores\n",
        "k_scores = []\n",
        "\n",
        "# loop throguh 1 to 50 to find optimal k\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
        "    k_scores.append(scores.mean())\n",
        "\n",
        "\n",
        "print(k_scores)\n",
        "print('Max of list', max(k_scores))\n",
        "\n",
        "# plot how accuracy changes as we vary k\n",
        "\n",
        "\n",
        "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
        "# plt.plot(x_axis, y_axis)\n",
        "x_ticks = []\n",
        "for i in range(1,50,2):\n",
        "  x_ticks.append(i);\n",
        "plt.plot(k_range, k_scores)\n",
        "plt.xticks(x_ticks)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Cross-validated accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsZqhgqXLm9Q"
      },
      "source": [
        "Convert the problem in Iris Data Set into a binary classification task (setosa versus non-setosa). We\n",
        "can do so by replacing the class labels of the instances to non-setosa except for those that belong\n",
        "to the setosa class.\n",
        "Create a training set that contains 80% of the labeled data and create a test set that contains the\n",
        "remaining 20%. Train a logistic regression classifier using the training set. Plot the ROC curve for\n",
        "the setosa class (positive class) when applying the logistic regression classifier to the test set.\n",
        "Submit the plot and the source code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvsD3YiYzWf"
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "#print(X)\n",
        "##print(y)\n",
        "\n",
        "#preprocess label\n",
        "for i in range(0,len(y) - 1):\n",
        "  if y[i] != 0:\n",
        "    y[i] = 1\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "#train our logistic model\n",
        "lg_classifier = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred = lg_classifier.predict(X_test)\n",
        "\n",
        "duration = (time.time() - start_time) * 1000\n",
        "# confidence score and running time:\n",
        "print(\"Knn running time milliseconds: %.2f\" % duration)\n",
        "error = (y_test != y_pred).sum()\n",
        "accuracy = (1 - error / y_test.shape[0]) * 100\n",
        "print('Misclassified samples: {}.'.format(error))\n",
        "print('train accuracy: %.2f' % lg_classifier.score(X_train, y_train))\n",
        "print(\"The Test accuracy is {:1.2f}%.\".format(accuracy))\n",
        "\n",
        "\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "# Compute ROC curve and ROC area for each class\n",
        "lr_probs = lg_classifier.predict_proba(X_test)\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "# summarize scores\n",
        "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
        "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MigoW-Mg93V"
      },
      "source": [
        "[Bonus question]\n",
        "\n",
        "spam or not spam using random forest and SVM\n",
        "\n",
        "The dataset has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%)\n",
        "spam messages. Please refer to https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection for\n",
        "more detailed descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCkh2s38iJMz"
      },
      "source": [
        "# load data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqvP0eziIzP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBeWs8gzhY8S"
      },
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7F9_z2Jjm8v"
      },
      "source": [
        "!unzip smsspamcollection.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-mUVeaijtQV"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_table('SMSSpamCollection',\n",
        "                   sep='\\t',\n",
        "                   header=None,\n",
        "                   names=['label', 'sms_message'])\n",
        "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
        "#print(df.head)\n",
        "\n",
        "X = df['sms_message']\n",
        "y = df['label']\n",
        "#replace example@gmail.com with email address\n",
        "X = X.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
        "                                 'emailaddress')\n",
        "#replace urls\n",
        "X = X.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
        "                                  'webaddress')\n",
        "\n",
        "# Remove punctuation\n",
        "X = X.str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "\n",
        "# change words to lower case\n",
        "X = X.str.lower()\n",
        "\n",
        "print(X.head)\n",
        "print(y.head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFxvHMKb6qrQ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6kXy92A6dkY"
      },
      "source": [
        "#generate features\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# create bag-of-words\n",
        "all_words = []\n",
        "\n",
        "for message in X:\n",
        "    words = word_tokenize(message)\n",
        "    for w in words:\n",
        "        all_words.append(w)\n",
        "        \n",
        "all_words = nltk.FreqDist(all_words)\n",
        "print('Total Number of words: {}'.format(len(all_words)))\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))\n",
        "#use top 2000 words as features\n",
        "word_features = list(all_words.keys())[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6UD2CgB_36-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# The find_features function will determine which of the 2000 word features are contained in the review\n",
        "\n",
        "def find_features(message):\n",
        "    words = word_tokenize(message)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "#features for all messages\n",
        "messages = list(zip(X, y))\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed = seed\n",
        "np.random.shuffle(messages)\n",
        "\n",
        "# call find_features function for each SMS message\n",
        "featuresets = [(find_features(text), label) for (text, label) in messages]\n",
        "npfeatures = np.array(featuresets)\n",
        "print(npfeatures[:,0])\n",
        "#print(npfeatures[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chzqZ76gm49x"
      },
      "source": [
        "#1 using SVM\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "\n",
        "training, testing = train_test_split(featuresets, test_size = 0.2, random_state=seed)\n",
        "#nptraining = np.array(training)\n",
        "#print(nptraining)\n",
        "svcModel = SklearnClassifier(SVC(kernel = 'linear'))\n",
        "\n",
        "start_time = time.time()\n",
        "svcModel.train(training)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#labels = svcModel.labels() #[0,1]\n",
        "#print(labels)\n",
        "# confidence score and running time:\n",
        "duration = (time.time() - start_time) * 1000\n",
        "print(\"svm running time milliseconds: %.2f\" % duration)\n",
        "\n",
        "#get predict_y labels\n",
        "txt_features, y_labels = zip(*testing)\n",
        "y_predict = svcModel.classify_many(txt_features)\n",
        "# print accuracy\n",
        "accuracy = nltk.classify.accuracy(svcModel, testing)*100\n",
        "print(\"SVC Accuracy: {}\".format(accuracy))\n",
        "# make class label prediction for testing set\n",
        "\n",
        "cf_matrix = confusion_matrix(y_labels, y_predict, labels=[0, 1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXzNS5sCpSRR"
      },
      "source": [
        "# plot confusion matrix for SVM\n",
        "import seaborn as sns\n",
        "group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXMBLxE8s-UC"
      },
      "source": [
        "#2 create random forest classifier\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "\n",
        "rfModel = SklearnClassifier(RandomForestClassifier())\n",
        "\n",
        "start_time = time.time()\n",
        "rfModel.train(training)\n",
        "\n",
        "#get predict_y labels\n",
        "txt_features, y_labels_rf = zip(*testing)\n",
        "y_predict_rf = rfModel.classify_many(txt_features)\n",
        "# print accuracy\n",
        "\n",
        "# confidence score and running time:\n",
        "duration = (time.time() - start_time) * 1000\n",
        "accuracy = nltk.classify.accuracy(rfModel, testing)*100\n",
        "cf_matrix_rf = confusion_matrix(y_labels_rf, y_predict_rf, labels=[0, 1])\n",
        "print(\"Random Forest Accuracy: {}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdQZpth_Kw_g"
      },
      "source": [
        "# draw confusion matrix for random forest\n",
        "\n",
        "import seaborn as sns\n",
        "group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix_rf, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}